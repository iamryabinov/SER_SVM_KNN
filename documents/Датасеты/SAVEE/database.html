<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0070)http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Database.html -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><!-- InstanceBegin template="/Templates/No Sidebars.dwt" codeOutsideHTMLIsLocked="false" --><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<!-- InstanceBeginEditable name="doctitle" -->
<title>Surrey Audio-Visual Expressed Emotion (SAVEE) Database</title>
<meta name="Description" content="Surrey Audio-Visual Expressed Emotion (SAVEE) Database">
<meta name="Keywords" content="SAVEE, audio-visual, multimodal, emotion, expression, affect, affective, classification, recognition, database">
<meta name="content-language" content="en-GB">
<link rel="icon" href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/images/FaceJK_icon.png" type="image/png">
<!-- InstanceEndEditable -->
<link href="./database_files/style.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="./database_files/jquery.min.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript" src="./database_files/hoverIntent.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript" src="./database_files/superfish.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript" src="./database_files/supersubs.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript"> 
    $(document).ready(function(){ 
        $("ul.sf-menu").supersubs({ 
            minWidth:    12,   // minimum width of sub-menus in em units 
            maxWidth:    27,   // maximum width of sub-menus in em units 
            extraWidth:  1     // extra width can ensure lines don't sometimes turn over 
                               // due to slight rounding differences and font-family 
        }).superfish();  // call supersubs first, then superfish, so that subs are 
                         // not display:none when measuring. Call before initialising 
                         // containing tabs for same reason. 
    }); 
</script>
<!-- InstanceBeginEditable name="head" -->
<!-- InstanceEndEditable -->
<style>._3emE9--dark-theme .-S-tR--ff-downloader{background:rgba(30,30,30,.93);border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#fff}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{background:#3d4b52}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#131415}._3emE9--dark-theme .-S-tR--ff-downloader ._10vpG--footer{background:rgba(30,30,30,.93)}._2mDEx--white-theme .-S-tR--ff-downloader{background:#fff;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#314c75}._2mDEx--white-theme .-S-tR--ff-downloader ._6_Mtt--header{font-weight:700}._2mDEx--white-theme .-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{border:0;color:rgba(0,0,0,.88)}._2mDEx--white-theme .-S-tR--ff-downloader ._10vpG--footer{background:#fff}.-S-tR--ff-downloader{display:block;overflow:hidden;position:fixed;bottom:20px;right:7.1%;width:330px;height:180px;background:rgba(30,30,30,.93);border-radius:2px;color:#fff;z-index:99999999;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);transition:.5s}.-S-tR--ff-downloader._3M7UQ--minimize{height:62px}.-S-tR--ff-downloader._3M7UQ--minimize .nxuu4--file-info,.-S-tR--ff-downloader._3M7UQ--minimize ._6_Mtt--header{display:none}.-S-tR--ff-downloader ._6_Mtt--header{padding:10px;font-size:17px;font-family:sans-serif}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{float:right;background:#f1ecec;height:20px;width:20px;text-align:center;padding:2px;margin-top:-10px;cursor:pointer}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#e2dede}.-S-tR--ff-downloader ._13XQ2--error{color:red;padding:10px;font-size:12px;line-height:19px}.-S-tR--ff-downloader ._2dFLA--container{position:relative;height:100%}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info{padding:6px 15px 0;font-family:sans-serif}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info div{margin-bottom:5px;width:100%;overflow:hidden}.-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{margin-top:21px;font-size:11px}.-S-tR--ff-downloader ._10vpG--footer{width:100%;bottom:0;position:absolute;font-weight:700}.-S-tR--ff-downloader ._10vpG--footer ._2V73d--loader{-webkit-animation:n0BD1--rotation 3.5s linear forwards;animation:n0BD1--rotation 3.5s linear forwards;position:absolute;top:-120px;left:calc(50% - 35px);border-radius:50%;border:5px solid #fff;border-top-color:#a29bfe;height:70px;width:70px;display:flex;justify-content:center;align-items:center}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar{width:100%;height:18px;background:#dfe6e9;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar ._1FVu9--progress-bar{height:100%;background:#8bc34a;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status{margin-top:10px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1XilH--state{float:left;font-size:.9em;letter-spacing:1pt;text-transform:uppercase;width:100px;height:20px;position:relative}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1jiaj--percentage{float:right}</style></head>
<body>
<div id="nav" class="container_12 rounded" style="text-align:center;">

<font size="+3"><font color="white">Surrey Audio-Visual Expressed Emotion (SAVEE) Database</font></font>
</div>

<div id="nav" class="container_12 rounded">
  <ul class="sf-menu sf-js-enabled sf-shadow">
    <li class="first-item"><a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/" style="border-left:none;">Home</a> </li>
    <li>  <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Introduction.html">Introduction</a>   </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Database.html">Database</a> </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Evaluation.html">Evaluation</a> </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/References.html">References</a> </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Download.html">Download</a> </li>
  </ul>
</div>

<div id="content" class="container_12 rounded">

  <div class="grid_12"><!-- InstanceBeginEditable name="MainContent" -->
      <h2><font color="blue">Corpus design</font></h2>
   
 <p> The SAVEE database was recorded from four native English male speakers (identified as DC, JE, JK, KL),
postgraduate students and researchers at the University of
Surrey aged from 27 to 31 years.
Emotion has been described psychologically in discrete categories: 
anger, disgust, fear, happiness, sadness and surprise.
This is supported by the cross-cultural studies of Ekman [6] and 
studies of automatic emotion recognition tended to focus on
recognizing these [12]. We added neutral to provide recordings of 7 emotion categories. The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phonetically-balanced. 
The 3 common and 2 × 6 = 12 emotion-specific sentences were recorded as neutral to
give 30 neutral sentences. This resulted in a total of 120 utterances per
speaker, for example: </p> <p>
<b>Common:</b> She had your dark suit in greasy wash water all year. <br>
<b>Anger:</b> Who authorized the unlimited expense account?  <br>
<b>Disgust:</b> Please take this dirty table cloth to the cleaners for me. <br>
<b>Fear:</b> Call an ambulance for medical assistance. <br>
<b>Happiness:</b> Those musicians harmonize marvelously. <br>
<b>Sadness:</b> The prospect of cutting back spending is an unpleasant one for any governor. <br>
<b>Surprise:</b> The carpet cleaners shampooed our oriental rug. <br>
<b>Neutral:</b> The best way to learn is to solve extra problems.  </p>
<p>
The distribution includes a complete list of sentences.
</p>
<hr>

    <h2><font color="blue">Data capture</font></h2>
    <p>The database was captured in CVSSP's 3D vision laboratory
over several months during different time period of the year
from four actors. The data capture setup is shown in Figure 1, and the Subjects recorded are shown in Figure 2. 
Emotion and text prompts were displayed on
a monitor in front of actors during the recordings. The emotion
prompts consisted of a video clip and three pictures for each
emotion. The data were recorded by dividing the text prompts
into three groups such that each group had sentences for each
emotion. The aim was to avoid the bias due to fatigue. Those
utterances for which the actor was unable to express proper
emotion were repeated until a satisfactory level was achieved.
The 3dMD dynamic face capture system [13] was used to capture
the 2D frontal color video and Beyerdynamic microphone signals. 
To extract facial expression features, the actors' frontal
face was painted with 60 markers. The markers were painted on
forehead, eyebrows, cheeks, lips and jaw, as shown in Figure 2.
To enable feature extraction for upper, middle and lower face
regions in Busso and Narayanan [11] markers were divided into
three groups, as shown in Figure 3 (right). The upper region
included markers above the eyes in the forehead and eyebrow
area. The lower region contained markers below the upper lip,
including the mouth and jaw. The middle region covered the
cheek area between the upper and lower regions. The placement
of facial markers in our work was inspired from Busso and
Narayanan [11]. The sampling rate was 44.1 kHz for audio and
60 fps for video. The total size of database is 480 utterances.</p> <br>


<table class="image">
<caption align="bottom">Figure 1: Data capture setup for SAVEE database.</caption>
<tbody><tr><td><img src="./database_files/DCapSet2-1.jpg" width="450"></td></tr>
</tbody></table> <br>

<table class="image">
<caption align="bottom">Figure 2: Blue markers for tracking placed on subjects' faces with various emotions (from left): KL (angry), JK (happy), JE (sad) and
DC (neutral).</caption>
<tbody><tr><td><img src="./database_files/DataRec.png" width="600" height="180"></td></tr>
</tbody></table> 

<hr>

    <h2><font color="blue">Data processing and annotation</font></h2>
    <h3><font color="blue">Speech data labeling</font></h3>
<p>
The speech data were labeled at phone level to extract duration
features, in a semi-automated way in two steps: 
first automatic labeling with the HTK software [14], second the Speech Filing
System (SFS) software [15] was used to correct labeling errors manually assisted by waveform and spectrogram displays, as shown in Figure 3 (left). The final
annotation gives phone-level labels for the audio data, for which the symbol set is described as part of the distribution.</p>

<h3><font color="blue">Marker tracking</font></h3>
<p>
To facilitate extraction of facial features, the actors' face was
painted with 60 markers. After data capture, markers were manually
labeled for the first frame of a sequence and then tracked
for the remaining frames using a marker tracker, as shown in Figure 3 (right). The marker
tracker thresholded pixels in the blue channel (i.e. the color of
the markers) and then detected blobs greater than a certain size.
This gave us a set of candidate markers, which were matched to
markers from the previous frame by looking for minimal overall
displacement, i.e. we found the closest markers between frames
n-1 and n, took that as a match and then proceeded to the next
closest until all markers have been tried. If there was no candidate
marker within a certain distance then that marker was
assumed to be occluded and was frozen in place until it became
visible again. This stage resulted a set of 2D marker coordinates
for each frame of the visual data.</p> <br>

<table class="image">
<caption align="bottom">Figure 3: Audio feature extraction with Speech Filing System software (left), and visual data (right) with tracked marker locations. Marker on the bridge of nose (encircled
in black) was taken as a reference.</caption>
<tbody><tr><td><img src="./database_files/speechfig.png" width="343"> <img src="./database_files/MarkerNew1.png" width="185"></td></tr>
</tbody></table> 

  <!-- InstanceEndEditable --></div>

  <div class="clear"></div>
</div>
<div id="footer" class="container_12 topborder">
  <div class="grid_6" style="text-align:left;"><a href="http://www.surrey.ac.uk/">University of Surrey</a> | 
<a href="http://www.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Register.html">Register</a> | 
<a href="mailto:p.jackson@surrey.ac.uk?subject=SAVEE">Contact Us</a>
<!-- <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/">Contact Us</a> -->
 </div>
</div>
<div id="credit" class="container_12">


  <!-- The following link and credit cannot legally be altered or removed without a valid website license key purchased from http://www.justdreamweaver.com/templates/license.html. License keys are only $19.99 and once purchased you are free to remove or alter the link. -->

<div class="grid_12">
Last update: 2 April 2015
<br>
Authors: Philip Jackson and Sanaul Haq
<br>
Designed by <a href="http://www.justdreamweaver.com/dreamweaver-templates.html" target="_blank">JustDreamweaver.com</a>

</div>
</div>

<!-- InstanceEnd -->
</body><div></div></html>