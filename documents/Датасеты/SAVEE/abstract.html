<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0074)http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Introduction.html -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><!-- InstanceBegin template="/Templates/No Sidebars.dwt" codeOutsideHTMLIsLocked="false" --><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<!-- InstanceBeginEditable name="doctitle" -->
<title>Surrey Audio-Visual Expressed Emotion (SAVEE) Database</title>
<meta name="Description" content="Surrey Audio-Visual Expressed Emotion (SAVEE) Database">
<meta name="Keywords" content="SAVEE, audio-visual, multimodal, emotion, expression, affect, affective, classification, recognition, database">
<meta name="content-language" content="en-GB">
<link rel="icon" href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/images/FaceJK_icon.png" type="image/png">
<!-- InstanceEndEditable -->
<link href="./abstract_files/style.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="./abstract_files/jquery.min.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript" src="./abstract_files/hoverIntent.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript" src="./abstract_files/superfish.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript" src="./abstract_files/supersubs.js.&#1041;&#1077;&#1079; &#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103;"></script>
<script type="text/javascript"> 
    $(document).ready(function(){ 
        $("ul.sf-menu").supersubs({ 
            minWidth:    12,   // minimum width of sub-menus in em units 
            maxWidth:    27,   // maximum width of sub-menus in em units 
            extraWidth:  1     // extra width can ensure lines don't sometimes turn over 
                               // due to slight rounding differences and font-family 
        }).superfish();  // call supersubs first, then superfish, so that subs are 
                         // not display:none when measuring. Call before initialising 
                         // containing tabs for same reason. 
    }); 
</script>
<!-- InstanceBeginEditable name="head" -->
<!-- InstanceEndEditable -->
<style>._3emE9--dark-theme .-S-tR--ff-downloader{background:rgba(30,30,30,.93);border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#fff}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{background:#3d4b52}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#131415}._3emE9--dark-theme .-S-tR--ff-downloader ._10vpG--footer{background:rgba(30,30,30,.93)}._2mDEx--white-theme .-S-tR--ff-downloader{background:#fff;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#314c75}._2mDEx--white-theme .-S-tR--ff-downloader ._6_Mtt--header{font-weight:700}._2mDEx--white-theme .-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{border:0;color:rgba(0,0,0,.88)}._2mDEx--white-theme .-S-tR--ff-downloader ._10vpG--footer{background:#fff}.-S-tR--ff-downloader{display:block;overflow:hidden;position:fixed;bottom:20px;right:7.1%;width:330px;height:180px;background:rgba(30,30,30,.93);border-radius:2px;color:#fff;z-index:99999999;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);transition:.5s}.-S-tR--ff-downloader._3M7UQ--minimize{height:62px}.-S-tR--ff-downloader._3M7UQ--minimize .nxuu4--file-info,.-S-tR--ff-downloader._3M7UQ--minimize ._6_Mtt--header{display:none}.-S-tR--ff-downloader ._6_Mtt--header{padding:10px;font-size:17px;font-family:sans-serif}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{float:right;background:#f1ecec;height:20px;width:20px;text-align:center;padding:2px;margin-top:-10px;cursor:pointer}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#e2dede}.-S-tR--ff-downloader ._13XQ2--error{color:red;padding:10px;font-size:12px;line-height:19px}.-S-tR--ff-downloader ._2dFLA--container{position:relative;height:100%}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info{padding:6px 15px 0;font-family:sans-serif}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info div{margin-bottom:5px;width:100%;overflow:hidden}.-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{margin-top:21px;font-size:11px}.-S-tR--ff-downloader ._10vpG--footer{width:100%;bottom:0;position:absolute;font-weight:700}.-S-tR--ff-downloader ._10vpG--footer ._2V73d--loader{-webkit-animation:n0BD1--rotation 3.5s linear forwards;animation:n0BD1--rotation 3.5s linear forwards;position:absolute;top:-120px;left:calc(50% - 35px);border-radius:50%;border:5px solid #fff;border-top-color:#a29bfe;height:70px;width:70px;display:flex;justify-content:center;align-items:center}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar{width:100%;height:18px;background:#dfe6e9;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar ._1FVu9--progress-bar{height:100%;background:#8bc34a;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status{margin-top:10px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1XilH--state{float:left;font-size:.9em;letter-spacing:1pt;text-transform:uppercase;width:100px;height:20px;position:relative}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1jiaj--percentage{float:right}</style></head>
<body>
<div id="nav" class="container_12 rounded" style="text-align:center;">

<font size="+3"><font color="white">Surrey Audio-Visual Expressed Emotion (SAVEE) Database</font></font>
</div>

<div id="nav" class="container_12 rounded">
  <ul class="sf-menu sf-js-enabled sf-shadow">
    <li class="first-item"><a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/" style="border-left:none;">Home</a> </li>
    <li>  <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Introduction.html">Introduction</a>   </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Database.html">Database</a> </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Evaluation.html">Evaluation</a> </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/References.html">References</a> </li>
    <li> <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Download.html">Download</a> </li>
  </ul>
</div>

<div id="content" class="container_12 rounded">

  <div class="grid_12"><!-- InstanceBeginEditable name="MainContent" -->
      <h2><font color="blue">Emotional Databases</font></h2>
    <p>Recent advancement in human-computer interaction (HCI)
technology goes beyond the successful transfer of data between
human and machine by seeking to improve the naturalness and
friendliness of user interactions. The user's expressed emotion
plays an important role in this regard. Speech is the primary
means of communication between human beings, and if confined
in meaning to the explicit verbal content of what is said,
it does not by itself carry all the information conveyed. Additional
information includes vocalized emotion, facial expressions,
hand gestures and body language [1] as well as biometric
indicators. In the design of an emotion recognition system, one
important factor upon which the performance of emotion recognition
system depends is the emotional database used to build
its representation of humans emotions.
</p>
<p>
Emotional behavior databases have been recorded for investigation
of emotion, where some are natural while others are
acted or elicited. These databases have been recorded in audio,
visual or audio-visual modalities. For the analysis of vocal expressions
of emotions, audio databases such as AIBO, Berlin
Database of Emotional Speech, and Danish Emotional Speech
Database have been recorded [2, 3, 4]. The AIBO database [2]
is a natural database which consists of recording from children
while interacting with robot. The database has 110 dialogues
and 29200 words in 11 emotion categories of anger, bored, emphatic,
helpless, ironic, joyful, motherese, reprimanding, rest,
surprise and touchy. The data labeling is based on listeners'
judgment. The Berlin Database of Emotional Speech [3] is
a German acted database, which consists of recordings from
10 actors (5 male, 5 female). The data consist of 10 German
sentences recorded in anger, boredom, disgust, fear, happiness,
sadness and neutral. The final database consists of 493 utterances
after listeners' judgment. The Danish Emotional Speech
Database [4] is another audio database recorded from 4 actors
(2 male, 2 female). The recorded data consist of 2 words, 9
sentences and 2 passages, resulting in 10 minutes of audio data.
The recorded emotions are anger, happiness, sadness, surprise
and neutral.
</p>
<p>
Some facial expressions databases have been recorded for
the analysis of facial emotional behavior. The Cohn-Kanade facial
expression database [5] is a popular acted database of facial
expressions, with recordings from 210 adults, in 6 basic emotions
[6] and Action Units (AUs). The data are labeled using the
Facial Action Coding System (FACS). The MMI database is a
very comprehensive data set of facial behavior [7]. It consists
of facial data for both acted and spontaneous expressions. The
recorded data comprise both static images and videos, where
large parts of the data are recorded in both frontal and profile
views of the face. For the natural data, children interacted with
a comedian, while adults responded to emotive videos. The
database consists of 1250 videos and 600 static images in 6
basic emotions, single AU and multiple AUs. The data labeling
is done by FACS and observers' judgment. The UT Dallas
database [8] is a natural visual database which is recorded
by asking subjects to watch emotion-inducing videos. The
database consists of data from 229 adults in 6 basic emotions,
along with puzzlement, laugh, boredom and disbelief. The data
labeling is based on observers' judgment.
</p>
<p>
In recent years, some audio-visual databases have been
recorded to investigate the importance of each modality and
different fusion techniques to improve the emotion recognition
performance. The Adult Attachment Interview (AAI) database
[9] is a natural audio-visual database, which consists of subjects'
interviews about their childhood experiences. The data
consist of recordings from 60 adults and each interview lasts
for 30-60 minutes. The database consists of the 6 basic emotions
along with embarrassment, contempt, shame, plus general
kinds of positive and negative emotion. The data labeling uses
FACS. The Belfast database [10] is another natural audio-visual
database which consists of clips taken from television and realistic
interviews conducted by a research team. The database
holds data from 125 subjects, as 209 sequences from TV and 30
from interviews. The data are labeled with both categorical and
dimensional emotion using the Feeltrace system. The Facial Motion
Capture database [11] consists of recordings from an
actress, who was asked to read a phoneme-balanced corpus four
times, expressing anger, happiness, sadness and neutral. The actress'
facial expression and rigid head motion were acquired by
a VICON motion capture system which captured the 3D positions' 
of 102 markers on her face. The total data consist of 612
sentences.
</p>
<p>
We wanted to design an audio-visual British English
database suitable for the development of a multimodal emotion
recognition system. We adopted a more controlled approach
than [9] and [10], including phonetically-balanced sentences
and 60 facial markers, to obtain phone-level annotations and coordinates
of points on the actors' faces. In comparison to [11],
we aimed to increase the number of actors and affect classes
to cover all the basic emotions with an even distribution. For
benchmarking purposes, we performed a subjective evaluation
of our database under audio-only, video-only and audio-visual
conditions, and report the classification results for both speaker-dependent
and speaker-independent baseline systems. 
</p>

  <!-- InstanceEndEditable --></div>

  <div class="clear"></div>
</div>
<div id="footer" class="container_12 topborder">
  <div class="grid_6" style="text-align:left;"><a href="http://www.surrey.ac.uk/">University of Surrey</a> | 
<a href="http://www.ee.surrey.ac.uk/Personal/P.Jackson/SAVEE/Register.html">Register</a> | 
<a href="mailto:p.jackson@surrey.ac.uk?subject=SAVEE">Contact Us</a>
<!-- <a href="http://personal.ee.surrey.ac.uk/Personal/P.Jackson/">Contact Us</a> -->
 </div>
</div>
<div id="credit" class="container_12">


  <!-- The following link and credit cannot legally be altered or removed without a valid website license key purchased from http://www.justdreamweaver.com/templates/license.html. License keys are only $19.99 and once purchased you are free to remove or alter the link. -->

<div class="grid_12">
Last update: 2 April 2015
<br>
Authors: Philip Jackson and Sanaul Haq
<br>
Designed by <a href="http://www.justdreamweaver.com/dreamweaver-templates.html" target="_blank">JustDreamweaver.com</a>

</div>
</div>

<!-- InstanceEnd -->
</body><div></div></html>